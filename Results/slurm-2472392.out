                     :-) GROMACS - mdrun, VERSION 5.1.4 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra   Sebastian Fritsch 
  Gerrit Groenhof   Christoph Junghans   Anca Hamuraru    Vincent Hindriksen
 Dimitrios Karkoulis    Peter Kasson        Jiri Kraus      Carsten Kutzner  
    Per Larsson      Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff 
   Erik Marklund      Teemu Murtola       Szilard Pall       Sander Pronk   
   Roland Schulz     Alexey Shvetsov     Michael Shirts     Alfons Sijbers  
   Peter Tieleman    Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      mdrun, VERSION 5.1.4
Executable:   /opt/gridunesp/dist/gromacs/5.1.4/bin/mdrun
Data prefix:  /opt/gridunesp/dist/gromacs/5.1.4
Command line:
  mdrun -deffnm md_0 -nb gpu


Back Off! I just backed up md_0.log to ./#md_0.log.1#

Running on 1 node with total 28 cores, 56 logical cores
Hardware detected:
  CPU info:
    Vendor: GenuineIntel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

Reading file md_0.tpr, VERSION 5.1.4 (single precision)
Changing nstlist from 10 to 20, rlist from 1 to 1.027


Will use 40 particle-particle and 16 PME only ranks
This is a guess, check the performance at the end of the log file
Using 56 MPI threads
Using 1 OpenMP thread per tMPI thread

starting mdrun 'Protein in water'
5000000 steps,  10000.0 ps.

NOTE: Turning on dynamic load balancing

srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 2472392.0 ON node011 CANCELLED AT 2021-05-22T09:44:31 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 2472392 ON node011 CANCELLED AT 2021-05-22T09:44:31 DUE TO TIME LIMIT ***
srun: got SIGCONT
srun: forcing job termination
rsync error: received SIGINT, SIGTERM, or SIGHUP (code 20) at rsync.c(638) [sender=3.1.2]
rsync error: received SIGINT, SIGTERM, or SIGHUP (code 20) at io.c(504) [generator=3.1.2]


Received the second INT/TERM signal, stopping at the next step


 Average load imbalance: 120.9 %
 Part of the total run time spent waiting due to load imbalance: 8.1 %
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 3 % Z 0 %
 Average PME mesh/force load: 1.525
 Part of the total run time spent waiting due to PP/PME imbalance: 17.7 %

NOTE: 8.1 % of the available CPU time was lost due to load imbalance
      in the domain decomposition.

NOTE: 17.7 % performance was lost because the PME ranks
      had more work to do than the PP ranks.
      You might want to increase the number of PME ranks
      or increase the cut-off and the grid spacing.


               Core t (s)   Wall t (s)        (%)
       Time:   276556.678    86396.087      320.1
                         23h59:56
                 (ns/day)    (hour/ns)
Performance:        0.315       76.128

gcq#223: "Jesus Not Only Saves, He Also Frequently Makes Backups." (Myron Bradshaw)

srun: error: node011: task 0: Exited with exit code 2
